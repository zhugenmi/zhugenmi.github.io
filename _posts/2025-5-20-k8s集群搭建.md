---
title: 基于OpenEuler 24.04的Kubernetes集群部署指南
author: zhugenmi
date: 2025-5-20 9:00 +0800
categories: [Systematic Capacity,TOOLS]
tags: [k8s]
description: 
---

# 基于OpenEuler 24.04的Kubernetes集群部署指南

Kubernetes 自 1.21 版本起已正式弃用 Docker 作为容器运行时，若需部署新集群，需采用符合 CRI（Container Runtime Interface）标准的替代方案。**本文将以轻量级运行时 Containerd 为核心，指导快速构建生产可用的 Kubernetes 集群。**如需自定义集群配置（如网络插件、存储驱动或高可用架构），请参阅 [Kubernetes 官方文档](https://kubernetes.io/zh-cn/docs/home/) 获取高级参数说明及最佳实践。

## 集群规划

**搭建目标**：基于华为 openEuler 24.03 LTS 系统，使用kubeadm v1.29.1—搭建一套由单 Master node 和两个 Worker node组成的 k8s v1.29.1 版本的集群环境。

在开始部署集群之前，要先做好系统环境准备，部署k8s集群的宿主机（vm或物理机）需要满足以下几个条件：

- 宿主机3台，其操作系统为openEuler 24.03 LTS；
- 硬件配置：RAM 4GB及以上，CPU核数2c及以上，硬盘40GB及以上；
- 集群中所有机器之间网络互通；
- 能够访问外网，需要拉取镜像；
- 防火墙关闭，同时禁止swap分区；
- 所有集群节点同步系统时间（使用`ntpdate`工具）。

操作系统使用[openEuler 24.03 LTS](https://www.openeuler.openatom.cn/zh/download/archive/detail/?version=openEuler%2024.03%20LTS)，openEuler 24.03 LTS 是基于6.6内核的长周期版本，面向服务器、云、边缘计算、AI和嵌入式场景，提供更多新特性和功能，给开发者和用户带来全新的体验，服务更多的领域和更多的用户。

> 注意：下面除特别注明外的所有的命令/配置都需在k8s的每一个节点执行。

使用 VMware Workstation Pro v17.6.2 虚拟机搭建 vm 环境，服务器规划如下：

| 主机名称    | 设备规格（内存/核数/硬盘） | ip地址             | 安装组件 | 操作系统        | k8s集群角色 |
| ----------- | -------------------------- | ------------------ | -------- | --------------- | ----------- |
| k8s-master  | 4GB/2/40GB                 | 192.168.174.128/24 |          | openEuler 24.03 | master      |
| k8s-node-01 | 4GB/2/40GB                 | 192.168.174.129/24 |          | openEuler 24.03 | worker node |
| k8s-node-02 | 4GB/2/40GB                 | 192.168.174.130/24 |          | openEuler 24.03 | worker node |

## 配置静态IP

使用`vi /etc/sysconfig/network-scripts/ifcfg-ens33`命令配置静态ip：

```ini
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=static  # 改为 static（静态IP）
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
NAME=ens33
UUID=xxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx  # 保持原样，不要修改
DEVICE=ens33  # 网卡名
ONBOOT=yes  # 开机自动启用
IPADDR=192.168.174.128  # 设置静态IP
PREFIX=24  # 子网掩码（24=255.255.255.0）
GATEWAY=192.168.174.2  # 与VMware网关一致
DNS1=8.8.8.8  # DNS 服务器
DNS2=119.29.29.29
```

如果使用虚拟机克隆，则还需要配置唯一的UUID：

```bash
sudo sed -i "s/UUID=.*/UUID=$(uuidgen)/" /etc/sysconfig/network-scripts/ifcfg-ens33
```

重启网络服务：

```bash
nmcli con reload  # 重新加载配置
nmcli con up ens33  # 激活网卡
```

安装必要软件包：

```bash
yum install openssh-server tar
```

修改`/etc/ssh/sshd_config`文件以允许端口转发：

```bash
vi /etc/ssh/sshd_config # 允许tcp端口转发 AllowTcpForwarding yes
```

保存后：

```bash
sudo systemctl restart sshd
```

## **设置主机名和hosts解析**

设置主机名：

```bash
# Master节点
sudo hostnamectl set-hostname k8s-master

# Node1节点
sudo hostnamectl set-hostname k8s-node-01

# Node2节点
sudo hostnamectl set-hostname k8s-node-02
```

所有节点使用命令`vi /etc/hosts`修改配置：

```plaintext
192.168.174.128 k8s-master
192.168.174.129 k8s-node-01
192.168.174.130 k8s-node-02
```

最后，通过以下命令验证：

```bash
# 1. 检查主机名
hostname

# 2. 检查节点间通信（每台机器执行）
ping k8s-master
ping k8s-node-01
ping k8s-node-02

# 3. 确认外网访问
ping www.baidu.com
```

安装`tar`和`sshd`：

```bash
yum install openssh-server tar
```

## 软件包安装

### 1. 安装必要软件包

**安装kubeadm/kubelet/kubectl**

```bash
# 添加Kubernetes源
cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=0
EOF
```

```bash
$ yum install -y kubernetes-kubelet-1.29.1-4.oe2403 kubernetes-kubeadm-1.29.1-4.oe2403 kubectl containerd cri-tools
```

> ![img](https://docs.openeuler.openatom.cn/zh/docs/24.03_LTS/docs/Kubernetes/public_sys-resources/icon-note.gif)**说明**
>
> - 如果系统中已经安装了Docker，请确保在安装containerd之前卸载Docker，否则可能会引发冲突。

`kubeadm`默认使用`containerd`连接到容器运行时（CRI），要求使用1.6.22-15或更高版本的containerd，如果下载的版本过低请运行以下命令升级成1.6.22-15版本，或自行升级。

在当前目录（/work），我已经下载好以下两个工具包：

```shell
cni-plugins-linux-amd64-v1.5.1.tgz  containerd-2.1.3-linux-amd64.tar.gz
```

本教程中通过yum下载的软件包版本如下所示：

```markdown
1. containerd
  -架构：x86_64
  -版本：2.1.3
2. kubernetes - client/help/kubeadm/kubelet/master/node
  -架构：x86_64
  -版本：1.29.1-4
3. cri-tools
  -架构：X86_64
  -版本：1.29.0-3
```

### 2. 下载cni组件

```shell
$ mkdir -p /opt/cni/bin
$ cd /opt/cni/bin
$ wget --no-check-certificate https://github.com/containernetworking/plugins/releases/download/v1.5.1/cni-plugins-linux-amd64-v1.5.1.tgz # 下载好对应的tgz文件
$ tar -xzvf ./cni-plugins-linux-amd64-v1.5.1.tgz -C .
```

### 3. 下载CNI插件(Flannel)

```shell
$ wget https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml --no-check-certificate
```

## 环境配置

本节对Kubernetes运行时所需的操作系统环境进行配置。

### 1. 设置主机名

```cpp
$ hostnamectl set-hostname k8s-master
```

### 2. 关闭防火墙

使用以下命令禁用防火墙：

```shell
$ systemctl stop firewalld
$ systemctl disable firewalld
```

### 3. 禁用SELinux

SELinux的安全策略可能会阻止容器内的某些操作，比如写入特定目录、访问网络资源、或执行具有特权的操作。这会导致 CoreDNS 等关键服务无法正常运行，并表现为CrashLoopBackOff或 Error状态。可以使用以下命令来禁用SELinux：

```shell
$ setenforce 0  # 临时禁用
$ sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config  # 永久禁用
```

### 4. 禁用swap

Kubernetes的资源调度器根据节点的可用内存和CPU资源来决定将哪些Pod分配到哪些节点上。如果节点上启用了swap，实际可用的物理内存和逻辑上可用的内存可能不一致，这会影响调度器的决策，导致某些节点出现过载，或者在某些情况下调度错误。因此需要禁用swap：

```shell
$ swapoff -a  # 临时关闭
$ sed -ri 's/.*swap.*/#&/' /etc/fstab  # 永久注释Swap
```

### 5. 网络配置

启用桥接网络上的IPv6和IPv4流量通过iptables进行过滤，并启动IP转发，运行内核转发IPv4包，确保跨界点的Pod间通信：

```bash
$ cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
vm.swappiness=0
EOF
# 加载内核模块
$ modprobe overlay # 可选
$ modprobe br_netfilter
$ sysctl -p /etc/sysctl.d/k8s.conf
$ echo -e "overlay\nbr_netfilter" | sudo tee /etc/modules-load.d/k8s.conf # 可选
```

这一步可能存在配置文件覆盖顺序问题，若不生效，使用`vim /etc/sysctl.d/99-sysctl.conf`删除掉`net.ipv4.ip_forward = 0`这一项即可。

### 6. 时间同步

```bash
sudo dnf install chrony -y
sudo systemctl enable --now chronyd
```

## 配置containerd

本节对containerd进行配置，包括设置pause_image、cgroup驱动、关闭"registry.k8s.io"镜像源证书验证、配置代理。

首先，生成containerd的默认配置文件并将其输出到containerd_conf指定的文件：

```shell
$ containerd_conf="/etc/containerd/config.toml"
$ mkdir -p /etc/containerd
$ containerd config default > "${containerd_conf}"
```

配置pause_image：

```shell
$ pause_img=$(kubeadm config ../assets/img/sysCapacity list | grep pause | tail -1)
$ sed -i "/sandbox_image/s#\".*\"#\"${pause_img}\"#" "${containerd_conf}" 
```

将cgroup驱动指定为systemd：

```shell
$ sed -i "/SystemdCgroup/s/=.*/= true/" "${containerd_conf}"
```

关闭"registry.k8s.io"镜像源证书验证：

```swift
$ sed -i '/plugins."io.containerd.grpc.v1.cri".registry.configs/a\[plugins."io.containerd.grpc.v1.cri".registry.configs."registry.k8s.io".tls]\n  insecure_skip_verify = true' /etc/containerd/config.toml
```

重启containerd，使得以上配置生效：

```ruby
$ systemctl daemon-reload
$ systemctl restart containerd
```

## 配置crictl使用containerd作为容器运行时

```ruby
$ crictl config runtime-endpoint unix:///run/containerd/containerd.sock
$ crictl config image-endpoint unix:///run/containerd/containerd.sock
```

## 配置kubelet使用systemd作为cgroup驱动

```shell
$ systemctl enable kubelet.service
$ echo 'KUBELET_EXTRA_ARGS="--runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice"'  >> /etc/sysconfig/kubelet
$ systemctl restart kubelet
```

**确认 cgroup 驱动一致性**，确保 kubelet 和容器运行时使用相同的驱动（如 `systemd`）：

```bash
kubeadm init phase kubelet-start # 生成默认配置文件

# 检查 kubelet 驱动
grep cgroupDriver /var/lib/kubelet/config.yaml

# 检查 containerd 驱动（若使用）
grep SystemdCgroup /etc/containerd/config.toml  # 应为 `true`

# 检查 CRI-O 驱动（若使用）
grep cgroup_manager /etc/crio/crio.conf        # 应为 `systemd`
```

## 初始化集群并加入节点

在**master节点**运行以下初始化命令：

```bash
kubeadm init --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/16 --apiserver-advertise-address=192.168.174.128 --image-repository registry.aliyuncs.com/google_containers 
```

- --apiserver-advertise-address 集群通告地址，这里为本机**master**节点的IP地址
- --image-repository 由于默认拉取镜像地址国内无法访问，这里指定阿里云镜像仓库地址
- --service-cidr 集群内部虚拟网络，Pod统一访问入口
- --pod-network-cidr Pod网络，，与之后部署的CNI网络组件`yaml`中保持一致

初始化成功后，会输出以下信息：

```bash
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.174.128:6443 --token y0d7oq.nwzfqo4cza49i7u8 \
        --discovery-token-ca-cert-hash sha256:cf784219f2418a810455608fc70274dcc46f88d1328859c9ceee5a5ea69c1cfd
```

根据该提示，在master节点依次执行：

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

export KUBECONFIG=/etc/kubernetes/admin.conf
```

执行完以上命令后，可以使用`kubectl get node`命令查看节点信息：

![image-20250703202520982](F:\Notes\Systematic capacity\../assets/img/sysCapacity\image-20250703202520982.png)

> ①是初始化后执行查看节点命令的结果，②是执行下面添加节点命令后的结果。

在**worker-node节点**运行上一步初始化成功后的命令：

```bash
kubeadm join 192.168.174.128:6443 --token y0d7oq.nwzfqo4cza49i7u8 \
        --discovery-token-ca-cert-hash sha256:cf784219f2418a810455608fc70274dcc46f88d1328859c9ceee5a5ea69c1cfd
```

![image-20250703202549593](F:\Notes\Systematic capacity\../assets/img/sysCapacity\image-20250703202549593.png)

## 配置网络插件

原理及方法见 [Kubernetes 安装网络插件(calico)](https://www.cnblogs.com/yangzp/p/16835280.html)。

下载Calico配置文件：

```bash
wget  --no-check-certificate   https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/calico.yaml
```

编辑配置文件`vim calico.yaml`，修改这几个地方：

```shell
            - name: CALICO_IPV4POOL_CIDR
              value: "10.244.0.0/16" # 与初始化命令kubeadm init 中的 --pod-network-cidr参数一致
            - name: IP_AUTODETECTION_METHOD
              value: "interface=ens33" # 与本地网卡名称一致
 .......
      containers:
        - name: calico-kube-controllers
          image: registry.cn-hangzhou.aliyuncs.com/ywflyfish/kube-controllers:v3.28.0 # 配置镜像地址
```

完成配置后，部署calico网络
```bash
kubectl apply -f calico.yaml
```

观察部署的实时情况：

```bash
watch kubectl get pods -n kube-system
```

等待几分钟查看pod情况：
```bash
kubectl get node
```

如果某节点显示`Init:ImagePullBackOff`的state，那么在该节点上配置镜像加速器，`vim /etc/containerd/config.toml`，在`[plugins."io.containerd.grpc.v1.cri".registry.mirrors]`选项下增加以下配置：

```bash
[plugins."io.containerd.grpc.v1.cri".registry.mirrors]
        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
          endpoint = ["https://docker.m.daocloud.io", "https://docker.mirrors.ustc.edu.cn", "https://hub-mirror.c.163.com"]
        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."quay.io"]
          endpoint = ["https://quay.mirror.aliyuncs.com"]
```

保存退出，重启`containerd`服务即可：

```bash
sudo systemctl restart containerd
```

![image-20250704115239720](F:\Notes\Systematic capacity\../assets/img/sysCapacity\image-20250704115239720.png)

## 安装metrics-server服务

metrics-server主要用于采集k8s集群资源信息。

```bash
wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.8.0/components.yaml
```

修改配置文件内容：

```shell
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=10250
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        - --kubelet-insecure-tls	# 增加此行
        image: registry.aliyuncs.com/google_containers/metrics-server:v0.8.0  # 修改镜像
```

然后安装服务：`kubectl  apply  -f  components.yaml`

等待1~2分钟，可以使用以下命令查看状态：

```bash
kubectl get pods -n kube-system -l k8s-app=metrics-server
```

按照完毕后，验证功能：

```bash
kubectl top nodes    # 查看节点资源使用
kubectl top pods -A  # 查看所有 Pod 资源
```

## 安装dashboard控制面板

下载`yaml`文件：

```bash
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

vim recommended.yaml
```

修改以下配置项：

```bash
spec:
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 30001 	#修改端口为30001
  type: NodePort		# 增加NodePort
  selector:
    k8s-app: kubernetes-dashboard
......
......
containers:
        - name: kubernetes-dashboard
          image: registry.cn-hangzhou.aliyuncs.com/ywflyfish/dashboard:v2.7.0 # 换国内镜像
          imagePullPolicy: Always
```

应用配置：

```bash
kubectl apply -f recommended.yaml

kubectl get ns # 查看pod情况
```

## 创建dashboard用户与登录

创建dashboard用户，下载文件：

```bash
wget https://raw.githubusercontent.com/cby-chen/Kubernetes/main/yaml/dashboard-user.yaml

kubectl apply -f  dashboard-user.yaml # 创建用户权限

kubectl get sa # 查看用户权限

kubectl create token admin-user  -n  kubernetes-dashboard # 创建用户token
```

保存创建的token，登录dashboard页面，地址：https://192.168.174.128:30001，输入token进行登录：

![image-20250704150429384](F:\Notes\Systematic capacity\../assets/img/sysCapacity\image-20250704150429384.png)

## 清除配置

```bash
# 1. 重置kubeadm安装（若之前初始化过集群）
sudo kubeadm reset -f
sudo systemctl stop kubelet cri-docker*

# 2. 卸载kubeadm/kubelet/kubectl
sudo dnf remove -y kubeadm kubelet kubectl cri-dockerd

# 3. 移除Kubernetes配置文件
sudo rm -rf /etc/kubernetes /var/lib/kubelet /etc/systemd/system/kubelet.service.d
sudo rm -rf /etc/cni /var/lib/etcd 
sudo rm -f /etc/systemd/system/kubelet.service.d/0-docker.conf

# 4. 卸载Docker及CRI适配器
sudo systemctl stop cri-docker.socket cri-docker.service docker
sudo dnf remove -y docker-ce docker-ce-cli containerd.io cri-dockerd
sudo rm -rf /etc/docker /var/lib/docker /etc/systemd/system/cri-docker.*

# 5. 清除残留数据
sudo rm -rf /var/run/cri-dockerd.sock /opt/cni/bin
sudo iptables -F && sudo iptables -t nat -F
```

> 部署过程中失败了很多次，一个是镜像拉取不成功，一个是每个节点UUID要不一致，这是k8s识别节点的依据，使用虚拟机-克隆来部署时尤其要注意这一点。

## Reference

[安装kubeadm](https://v1-32.docs.kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)

[使用 kubectl 创建 Deployment](https://kubernetes.io/zh-cn/docs/tutorials/kubernetes-basics/deploy-app/deploy-intro/)

[openEuler-Kubernetes集群部署指南 - containerd](https://docs.openeuler.openatom.cn/zh/docs/24.03_LTS/docs/Kubernetes/Kubernetes%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97-containerd.html)

[Ubuntu 22.04 搭建K8s集群](https://www.cnblogs.com/way2backend/p/16970506.html)

[官方文档-使用 kubeadm 引导集群](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/)

[Kubeadm 快速搭建 k8s v1.24.1 集群（openEuler 22.03 LTS）](https://mp.weixin.qq.com/s?__biz=Mzg4MTg0MjIxMw==&mid=2247484054&idx=4&sn=17e159ea32c73c561b642a2ab66fd3a6&chksm=cf5e83dcf8290acaec78622542527dfe2d2e3347374dd0893c2b88732daec57542e7e496b456#rd)

[KubeKey 部署 K8s v1.28.8 实战](https://mp.weixin.qq.com/s?src=11&timestamp=1750943679&ver=6076&signature=u-8ttfGsefzgaBpJnjrfX5ph48cNHcPC4lvqdf6gEA9sWO8E6wcC9PStZGooTroL*QpEIjR1iXZw2kI6y4RVybxmTOmNQRT4Dg9tXV86HQRahv0CebYjcCESxdcDGQXI&new=1)

[Kubernetes 安装网络插件(calico)](https://www.cnblogs.com/yangzp/p/16835280.html)

[openEuler24.03安装k8s v1.32.2，并设sc默认供应链](https://blog.51cto.com/u_14016919/13497499)

[基于Kubernetes 1.22搭建基准测试系统(train-ticket)手册](https://blog.csdn.net/zhtianxing/article/details/131455106)

[Ubuntu24使用kubeadm部署高可用K8S集群](https://www.cnblogs.com/lldhsds/p/18261304)

# 加入外部服务器到集群中

### 加入 Kubernetes 集群的完整流程

#### 1. 网络准备（关键步骤）

```bash
# 在 k8s-master 上添加路由（确保能访问 192.168.1.0/24 网段）
sudo ip route add 192.168.1.0/24 via 192.168.174.2 dev ens33

# 在 Ubuntu 服务器上添加路由（确保能访问 192.168.174.0/24 网段）
ssh user@192.168.1.100
sudo ip route add 192.168.174.0/24 via 192.168.1.1 dev ens160  # 替换为实际网关和接口

# 验证双向网络连通性
ping 192.168.174.128  # 从 Ubuntu 服务器 ping master
ping 192.168.1.100    # 从 master ping Ubuntu 服务器
```

```bash
#!/bin/bash
# 1. 停止 Kubernetes 相关服务
sudo systemctl stop kubelet containerd

# 2. 重置 kubeadm 安装
sudo kubeadm reset -f

# 3. 卸载 Kubernetes 组件
sudo apt purge -y kubeadm kubelet kubectl kubernetes-cni
sudo apt autoremove -y

# 4. 清理配置文件（避免删除关键网络配置）
sudo rm -rf /etc/kubernetes
sudo rm -rf /var/lib/kubelet
sudo rm -rf /var/lib/etcd
sudo rm -rf $HOME/.kube
sudo rm -f /etc/systemd/system/kubelet.service
sudo rm -rf /etc/systemd/system/kubelet.service.d

# 5. 清理容器运行时（可选）
## 如果不卸载 containerd，可以跳过此步
sudo systemctl stop containerd
sudo apt purge -y containerd runc
sudo rm -rf /var/lib/containerd
sudo rm -rf /etc/containerd

# 6. 安全清理网络配置
## 只删除 CNI 相关配置，不清理核心网络规则
sudo rm -rf /etc/cni/net.d/*
sudo rm -rf /opt/cni/bin/*
sudo ip link delete cni0 2>/dev/null
sudo ip link delete flannel.1 2>/dev/null

## 安全清理 iptables（只删除 Kubernetes 相关规则）
sudo iptables-save | grep -v KUBE- | grep -v CNI- | sudo iptables-restore
sudo ipvsadm --clear

# 7. 清理残留文件
sudo find /var/lib -name '*kube*' -exec rm -rf {} + 2>/dev/null
sudo find /etc -name '*kube*' -exec rm -rf {} + 2>/dev/null
sudo docker rm -f $(docker ps -aq) 2>/dev/null
sudo docker rmi -f $(docker ../assets/img/sysCapacity -q) 2>/dev/null
sudo crictl rm -fa 2>/dev/null
sudo crictl rmi -a 2>/dev/null

# 8. 重启系统（重要！恢复网络配置）
echo "清理完成，建议重启系统以恢复网络配置"
echo "执行: sudo reboot"
```

如果`kubelet`、`kubeadm` 和 `kubectl` 被标记为 **held**（锁定）状态，导致 `apt` 无法直接卸载它们。需要先解除锁定，然后再卸载。

```bash
sudo apt-mark unhold kubelet kubeadm kubectl # 解除锁定
sudo apt-get remove -y kubelet kubeadm kubectl
sudo apt-get autoremove -y # 清理相关依赖
sudo rm -rf /etc/kubernetes # 删除配置文件
sudo rm -rf /var/lib/kubelet
sudo rm -rf ~/.kube
```

